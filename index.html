
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" href="https://cdn.creazilla.com/icons/3212952/robot-icon-lg.png" type="image/vnd.microsoft.icon" />
  <title>Nishad Gothoskar</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="75%" valign="middle">
              <p align="center">
                <name>Nishad Gothoskar</name>
              </p>
        <p >I am currently at a stealth startup, where I am working on using probabilistic programming to scale up 3D perception.
          I recieved my PhD in Computer Science at MIT advised by <a href="http://probcomp.csail.mit.edu/principal-investigator/">Vikash Mansinghka</a> and <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>. 
          My goal is to build AI vision systems that can learn as rapidly and generalize as broadly as humans.
          Checkout my thesis <a href="https://www.youtube.com/watch?v=6x4QqHCbacc">here</a>.
        <br><br>
          <div style="text-align: center;">
            <a href="https://scholar.google.com/citations?user=RikRyq4AAAAJ&hl=en" target="_blank">
              <img src="https://upload.wikimedia.org/wikipedia/commons/c/c7/Google_Scholar_logo.svg" alt="Google Scholar" style="width: 32px; height: 32px; margin: 0 10px;">
            </a>
            <a href="https://www.linkedin.com/in/nishadg/" target="_blank">
              <img src="https://upload.wikimedia.org/wikipedia/commons/c/ca/LinkedIn_logo_initials.png" alt="LinkedIn" style="width: 32px; height: 32px; margin: 0 10px;">
            </a>
            <a href="https://github.com/nishadgothoskar" target="_blank">
              <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="width: 32px; height: 32px; margin: 0 10px;">
            </a>
            <a href="https://www.instagram.com/nish.d.g" target="_blank">
              <img src="https://upload.wikimedia.org/wikipedia/commons/e/e7/Instagram_logo_2016.svg" alt="Instagram" style="width: 32px; height: 32px; margin: 0 10px;">
            </a>
            <a href="https://mailhide.io/e/yZS51sEd">
              <img src="https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg" alt="Email" style="width: 32px; height: 32px; margin: 0 10px;">
            </a>
          </div>
            </td>
            <td width="33%">
              <img src="assets/website.png" width="350" height="350">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="33%" align="center">
              <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Google_%22G%22_logo.svg/640px-Google_%22G%22_logo.svg.png" width="100" height="100">
              <p>
                <strong>Google</strong><br>
                SWE Intern<br>
                2016<br>
              </p>
            </td>
            <td width="33%" align="center">
              <img src="https://images.crunchbase.com/image/upload/c_pad,h_160,w_160,f_auto,b_white,q_auto:eco,dpr_3/dlzugstswtlgq9wejmfi" width="100" height="100">
              <p>
                <strong>Uber ATG</strong><br>
                Software Engineer<br>
                2017 - 2018<br>
              </p>
            </td>
            <td width="33%" align="center">
              <img src="assets/vicarious_logo.png" width="100" height="100">
              <p>
                <strong>Vicarious AI</strong><br>
                Research Engineer<br>
                2018 - 2020<br>
              </p>
            </td>
            <td width="33%" align="center">
              <img src="https://upload.wikimedia.org/wikipedia/commons/0/0c/MIT_logo.svg" width="130" height="60" style="padding: 20px;">
              <p>
                <strong>MIT</strong><br>
                Graduate Student<br>
                2020 - 2025<br>
              </p>
            </td>
            <td width="33%" align="center">
              <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Icon-round-Question_mark.svg/2048px-Icon-round-Question_mark.svg.png" width="100" height="100">
              <p>
                <strong>Stealth Startup</strong><br>
                Co-founder
                <br>
                2025 - <br>
              </p>
            </td>


          </tr>
        </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td width="100%" valign="middle">
      <heading>Publication Highlights</heading>
    </td>
  </tr>
</table>

<!-- 

Gen3D: Real-Time 3D Perception with Probabilistic Programs
	Gothoskar*, Matheos* et. al. in prep
3DP3: 3D scene perception via probabilistic programming
	Gothoskar et. al. NeurIPS 2021
Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps
	George, Rikhye, Gothoskar et. al. Nature Communications
Bayes3D: fast learning and inference in structured generative models of 3D objects and scenes
	Gothoskar*, Ghavami*, Li* et. al. arXiv
DURableVS: Data-efficient Unsupervised Recalibrating Visual Servoing
	Gothoskar et. al. ICRA 2022
3DNEL: 3D Neural Embedding Likelihoods
	Zhou*, Gothoskar* et. al. ICCV 2023
SMCP3: Sequential monte carlo with probabilistic program proposals
	Lew, Matheos, Zhi-Xuan, Ghavami, Gothoskar, et. al. ICRA 2022
TAMPURA: Partially observable TAMP with uncertainty and risk awareness
	Curtis, Matheos, Gothoskar et. al. RSS 2024
Query training: Learning a worse model to infer better marginals in undirected graphical models with hidden variables
	LÃ¡zaro-Gredilla, Lehrach, Gothoskar et. al. AAAI 2021

   -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


  <tr>
    <td width="25%">
      <div class="one">
        <div class="two"><img src='assets/b3d_gif.gif' width="160" vspace="20"></div>
      </div>
    </td>
    <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2111.00312"><papertitle>Bayes3D: fast learning and inference in structured generative models of 3D objects and scenes
        </papertitle></a>
      <br>
      <strong>Nishad Gothoskar</strong>, Matin Ghavami, Eric Li, Aidan Curtis, Michael Noseworthy, Karen Chung, Brian Patton, William T. Freeman, Joshua B. Tenenbaum, Mirko Klukas, Vikash K. Mansinghka
      <br>
      [<a href="https://arxiv.org/pdf/2312.08715">PDF</a>]
      <br>
      <p></p>
      <p>We propose a generative probabilistic programming-based architecture for modeling 3D objects and scenes, and use our architecture to do accurate and robust object pose estimation from RGBD images.</p>
    </td>
  </tr>



  <tr>
    <td width="25%">
      <div class="one">
        <div class="two"><img src='assets/3dp3.png' width="160" vspace="20"></div>
      </div>
    </td>
    <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2111.00312"><papertitle>3DP3: 3D Scene Perception via Probabilistic Programs</papertitle></a>
      <br>
      <strong>Nishad Gothoskar</strong>, Marco Cusumano-Towner, Ben Zinberg, Matin Ghavamizadeh, Falk Pollok, Austin Garrett, Dan Gutfreund, Joshua B. Tenenbaum, Vikash Mansinghka
      <br>
      <em>NeurIPS</em>, 2021 [<a href="https://news.mit.edu/2021/probablistic-programming-machine-vision-1208">MIT News</a>]
      [<a href="https://arxiv.org/pdf/2111.00312.pdf">PDF</a>]
      <br>
      <p></p>
      <p>We propose a generative probabilistic programming-based architecture for modeling 3D objects and scenes, and use our architecture to do accurate and robust object pose estimation from RGBD images.</p>
    </td>
  </tr>


  <tr>
    <td width="25%">
      <div class="one">
        <div class="two"><img src='assets/cscg.png' width="160" vspace="20"></div>
      </div>
    </td>
    <td valign="middle" width="75%">
    <a href="https://www.nature.com/articles/s41467-021-22559-5">
        <papertitle>Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps</papertitle>
    </a>
      <br>
      Dileep George, Rajeev V. Rikhye, <strong>Nishad Gothoskar</strong>,J. Swaroop Guntupalli, Antoine Dedieu, Miguel Lazaro-Gredilla
      <br>
      <em>Nature Communications</em>, 2021 [<a href="https://www.nature.com/articles/s41467-021-22559-5.pdf">PDF</a>]
      <br>
      <p></p>
      <p>Cognitive maps are mental representations of spatial and conceptual relationships in an environment, and are critical for flexible behavior. To form these abstract maps, the hippocampus has to learn to separate or merge aliased observations appropriately in different contexts in a manner that enables generalization and efficient planning. Here we propose a specific higher-order graph structure, clone-structured cognitive graph (CSCG), which forms clones of an observation for different contexts as a representation that addresses these problems.</p>
    </td>
  </tr>


  <tr>
    <td width="25%">
      <div class="one">
        <div class="two"><img src='assets/dvs.png' width="160" vspace="20"></div>
      </div>
    </td>
    <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2202.03697"><papertitle>DURableVS: Data-efficient Unsupervised Recalibrating Visual Servoing via online learning in a structured generative model</papertitle></a>
      <br>
      <strong>Nishad Gothoskar</strong>, Miguel Lazaro-Gredilla, Yasemin Bekiroglu, Abhishek Agarwal, Joshua B. Tenenbaum, Vikash K. Mansinghka, Dileep George
      <br>
      <em>ICRA</em>, 2022
      [<a href="https://arxiv.org/pdf/2202.03697.pdf">PDF</a>]
      <br>
      <p></p>
      <p>In this work, we present a method for unsupervised learning of visual servoing that does not require any prior calibration and is extremely data-efficient. Our key insight is that visual servoing does not depend on identifying the veridical kinematic and camera parameters, but instead only on an accurate generative model of image feature observations from the joint positions of the robot. We demonstrate that with our model architecture and learning algorithm, we can consistently learn accurate models from less than 50 training samples (which amounts to less than 1 min of unsupervised data collection), and that such data-efficient learning is not possible with standard neural architectures. Further, we show that by using the generative model in the loop and learning online, we can enable a robotic system to recover from calibration errors and to detect and quickly adapt to possibly unexpected changes in the robot-camera system (e.g. bumped camera, new objects).</p>
    </td>
  </tr>

  
</table>

  <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
      <tbody><tr>
        <td>
          <br>
          <p align="center">
              Template from <a href="https://github.com/jonbarron/jonbarron_website">here.</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>

</body>

</html>
